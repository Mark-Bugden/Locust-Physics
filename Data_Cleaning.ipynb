{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9af267-67e0-4721-8e88-cb4788eac5a5",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "This notebook takes as input a raw .tsv file from the motion tracking software, and converts it to a cleaned file to be used for later data analysis.\n",
    "For now, it imports the entire tsv file, which will be a problem when we want to clean the bigger files. \n",
    "\n",
    "The notebook is set up as follows:\n",
    "1. Importing packages and reading data\n",
    "2. Renaming columns, removing unwanted data, separating locusts and arena markers\n",
    "3. Finding radius and center of arena\n",
    "4. Shifting and rescaling position coordinates of locusts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71b635-234b-4c86-9dcd-5ea1a545fa76",
   "metadata": {},
   "source": [
    "## 1. Importing packages and reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eddf148-58c8-43ed-b549-d6fa8bab2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  #The main package we will be using for data analysis. Used to read CSVs/TSVs, work with dataframe objects, and many other things.\n",
    "import glob, os #We might need this to deal with filenames and paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54db939a-26be-4d84-9d42-5a4226973c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the location of the data folder on your computer\n",
    "\n",
    "data_location = 'D:/Data/Hangar-Locust/'\n",
    "subfolder = '2K-locust_15112022/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "324f74d3-aa4b-4490-a8ad-6348ee905d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(dataframe):\n",
    "    \n",
    "\n",
    "\n",
    "    # The first three x,y,z columns should be renamed to x0,y0,z0.\n",
    "    clean_df = dataframe.copy()\n",
    "    clean_df = clean_df.rename(columns={\" X\": \" X.0\", \" Y\": \" Y.0\", \" Z\": \" Z.0\"})\n",
    "\n",
    "    # We also want to get rid of any annoying spaces in the column names\n",
    "    clean_df = clean_df.rename(str.strip, axis='columns')\n",
    "    \n",
    "    \n",
    "    # We notice that a lot of the entries are NaN. We would like to get rid of the columns where > 90% of the data is NaN. \n",
    "    # We want to delete any columns that have more than 90% NaN. \n",
    "    # First, find the columns we want to delete\n",
    "    cols_to_delete = clean_df.columns[clean_df.isnull().sum()/len(df) > .90]\n",
    "    \n",
    "    # It's possible, although unlikely, that the previous command found a column like 'X.420' with more than 90% NaN, but the corresponding columns 'Y.420' and/or 'Z.420' weren't included. \n",
    "    # To check this, we will go through this list of columns, and check that whenever there is a column there, all three (X,Y, and Z) of them are there. \n",
    "    cols_to_delete_list = [int(cols[2:]) for cols in cols_to_delete[:-1]]\n",
    "    everything_ok = True\n",
    "    for col in cols_to_delete_list:\n",
    "        if (cols_to_delete_list.count(col)) != 3:\n",
    "            everything_ok = False\n",
    "\n",
    "        \n",
    "    # Now we can remove the columns from the dataframe (as long as everything is ok)\n",
    "    if everything_ok == True:\n",
    "        clean_df.drop(cols_to_delete, axis = 1, inplace = True)\n",
    "        \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc4f5b-5bc9-4449-a411-34caa18b5f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file 255_locust_11152022_0014 Backup 2022-11-16 09.56.45.tsv because it is too large\n",
      "Skipping file 255_locust_11152022_0014.tsv because it is too large\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_location+subfolder)\n",
    "for file in glob.glob(\"*.tsv\"):\n",
    "    \n",
    "    # Only read in files that are smaller than 4GB. This is because I don't have a lot of RAM on my computer. \n",
    "    if os.stat(file).st_size/(1024*1024*1024) < 2:\n",
    "        \n",
    "        # Reads the .tsv file as a pandas DataFrame. We skip the first 11 rows because that is the metadata at the start, and we choose the Time column to be our index column. \n",
    "        df = pd.read_csv(data_location + subfolder + file, sep='\\t', skiprows=11, index_col='Time')\n",
    "        clean_df = clean_dataframe(df)\n",
    "\n",
    "        # Save the dataframe as a clean .tsv file for later use.\n",
    "        clean_df.to_csv(data_location + 'cleaned/' + f'cleaned_{subfolder}' + f'cleaned_{file}', sep='\\t')\n",
    "    else:\n",
    "        print(f'Skipping file {file} because it is too large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf8e06-f53b-4bda-8b11-3542cc6281bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef876f-5d52-4e08-8a20-ad140c47c59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
